{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276a46d3",
   "metadata": {},
   "source": [
    "Merging the datasets into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463fd101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from typing import Optional\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_merge_data(sentiment_file: str, trader_file: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads, cleans, and merges trader and sentiment data into a single DataFrame.\n",
    "\n",
    "    This function handles the core preprocessing pipeline. It loads \n",
    "    high-frequency trader data and daily market sentiment. It robustly \n",
    "    parses multiple timestamp formats (IST strings and Unix ms) from the \n",
    "    trader data.\n",
    "    \n",
    "    It then filters for relevant trades (only those with a non-zero PnL) \n",
    "    and merges the daily sentiment onto each individual trade using the \n",
    "    'date' as a common key. Missing sentiment values (e.g., on weekends) \n",
    "    are forward-filled.\n",
    "\n",
    "    Args:\n",
    "        sentiment_file (str): \n",
    "            Filepath for the sentiment data (e.g., 'fear_greed_index.csv').\n",
    "        trader_file (str): \n",
    "            Filepath for the historical trader data (e.g., 'historical_data.csv').\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: \n",
    "            A fully preprocessed and merged DataFrame containing one row \n",
    "            per relevant trade, with corresponding sentiment data.\n",
    "            Returns `None` if files cannot be loaded or if no relevant \n",
    "            trades are found after filtering.\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        df_sentiment = pd.read_csv(sentiment_file)\n",
    "        df_trader = pd.read_csv(trader_file)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Make sure files are in the correct path.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Preprocessing...\")\n",
    "    \n",
    "    # --- 1. Preprocess Sentiment Data ---\n",
    "    \n",
    "    # Standardize sentiment 'date' column to datetime objects for merging\n",
    "    df_sentiment['date'] = pd.to_datetime(df_sentiment['date'])\n",
    "    \n",
    "    # Select and rename relevant sentiment columns for clarity and easier access\n",
    "    df_sentiment = df_sentiment[['date', 'value', 'classification']]\n",
    "    df_sentiment = df_sentiment.rename(columns={'value': 'sentiment_value', \n",
    "                                              'classification': 'sentiment_class'})\n",
    "\n",
    "    # --- 2. Preprocess Trader Data (Robust Timestamp Conversion) ---\n",
    "    \n",
    "    # First, attempt to parse the 'Timestamp IST' string (DD-MM-YYYY format)\n",
    "    df_trader['Timestamp'] = pd.to_datetime(df_trader['Timestamp IST'], dayfirst=True, errors='coerce')\n",
    "    \n",
    "    # Identify rows where the 'Timestamp IST' parsing failed (resulted in NaT)\n",
    "    failed_mask = df_trader['Timestamp'].isna()\n",
    "    \n",
    "    # For *only* the failed rows, attempt to parse the numeric 'Timestamp' column (Unix ms)\n",
    "    if failed_mask.any():\n",
    "        print(f\"Reparsing {failed_mask.sum()} rows using Unix timestamp...\")\n",
    "        # .loc ensures we only update the failed rows, preserving the successfully parsed ones\n",
    "        df_trader.loc[failed_mask, 'Timestamp'] = pd.to_datetime(\n",
    "            df_trader.loc[failed_mask, 'Timestamp'], unit='ms', errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Drop any rows that failed *both* parsing attempts\n",
    "    df_trader = df_trader.dropna(subset=['Timestamp'])\n",
    "    \n",
    "    # Create the 'date' merge key by normalizing the timestamp (sets time to 00:00:00)\n",
    "    df_trader['date'] = df_trader['Timestamp'].dt.normalize()\n",
    "    \n",
    "    # --- 3. Filter for Relevant Trades & Convert Types ---\n",
    "    \n",
    "    # Ensure key financial columns are numeric, coercing any non-numeric values to NaN\n",
    "    numeric_cols = ['Closed PnL', 'Size USD', 'Execution Price']\n",
    "    for col in numeric_cols:\n",
    "        df_trader[col] = pd.to_numeric(df_trader[col], errors='coerce')\n",
    "    \n",
    "    # Create the modeling dataset by selecting *only* closing trades (where PnL is non-zero)\n",
    "    # This filters out open positions or other non-trade events.\n",
    "    df_model_data = df_trader[df_trader['Closed PnL'] != 0].copy()\n",
    "    \n",
    "    # Drop rows with missing critical values (PnL, Size, Account)\n",
    "    df_model_data = df_model_data.dropna(subset=['Closed PnL', 'Size USD', 'Account'])\n",
    "    \n",
    "    if df_model_data.empty:\n",
    "        print(\"Error: No data remaining after filtering for non-zero PnL.\")\n",
    "        return None\n",
    "\n",
    "    # --- 4. Merge Datasets ---\n",
    "    print(\"Merging datasets...\")\n",
    "    \n",
    "    # --- Debugging: Confirm 'date' key exists in both DataFrames before merging ---\n",
    "    print(f\"Columns in df_model_data: {df_model_data.columns.to_list()}\")\n",
    "    print(f\"Columns in df_sentiment: {df_sentiment.columns.to_list()}\")\n",
    "    # --------------------------------------------------------------------------\n",
    "    \n",
    "    # Perform a left merge: keep all trades, add sentiment data where it exists\n",
    "    df_merged = pd.merge(df_model_data, df_sentiment, on='date', how='left')\n",
    "    \n",
    "    # Forward-fill missing sentiment values. This is crucial for trades on\n",
    "    # weekends or holidays when the sentiment index might not update.\n",
    "    df_merged['sentiment_value'] = df_merged['sentiment_value'].ffill()\n",
    "    df_merged['sentiment_class'] = df_merged['sentiment_class'].ffill()\n",
    "    \n",
    "    # Drop any remaining rows where sentiment could not be filled (e.g., very old trades)\n",
    "    df_merged = df_merged.dropna()\n",
    "    \n",
    "    print(f\"Data ready. Total relevant trades for modeling: {len(df_merged)}\")\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e297776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Step 1: Load and Merge Data ---\n",
      "Loading data...\n",
      "Preprocessing...\n",
      "Merging datasets...\n",
      "Columns in df_model_data: ['Account', 'Coin', 'Execution Price', 'Size Tokens', 'Size USD', 'Side', 'Timestamp IST', 'Start Position', 'Direction', 'Closed PnL', 'Transaction Hash', 'Order ID', 'Crossed', 'Fee', 'Trade ID', 'Timestamp', 'date']\n",
      "Columns in df_sentiment: ['date', 'sentiment_value', 'sentiment_class']\n",
      "Data ready. Total relevant trades for modeling: 104408\n",
      "\n",
      "Successfully loaded and merged data.\n",
      "                                      Account  Coin  Execution Price  \\\n",
      "0  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           9.0570   \n",
      "1  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           9.0570   \n",
      "2  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           9.0480   \n",
      "3  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           9.0464   \n",
      "4  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           9.0424   \n",
      "\n",
      "   Size Tokens  Size USD  Side     Timestamp IST  Start Position Direction  \\\n",
      "0      2446.39  22156.95  SELL  03-12-2024 14:39    10000.498600      Sell   \n",
      "1        25.00    226.43  SELL  03-12-2024 14:39     7554.108602      Sell   \n",
      "2        33.40    302.20  SELL  03-12-2024 14:39     7529.108602      Sell   \n",
      "3       200.00   1809.28  SELL  03-12-2024 14:39     7495.708602      Sell   \n",
      "4       209.26   1892.21  SELL  03-12-2024 14:39     7295.708602      Sell   \n",
      "\n",
      "    Closed PnL                                   Transaction Hash  \\\n",
      "0  3008.231185  0x7b57a76296641058064004184ed84d01d0005daded34...   \n",
      "1    30.741534  0x7b57a76296641058064004184ed84d01d0005daded34...   \n",
      "2    40.770089  0x7b57a76296641058064004184ed84d01d0005daded34...   \n",
      "3   243.812267  0x7b57a76296641058064004184ed84d01d0005daded34...   \n",
      "4   254.263735  0x7b57a76296641058064004184ed84d01d0005daded34...   \n",
      "\n",
      "      Order ID  Crossed       Fee      Trade ID           Timestamp  \\\n",
      "0  52200758791     True  7.754933  5.190000e+14 2024-12-03 14:39:00   \n",
      "1  52200758791     True  0.079248  3.350000e+14 2024-12-03 14:39:00   \n",
      "2  52200758791     True  0.105771  5.140000e+14 2024-12-03 14:39:00   \n",
      "3  52200758791     True  0.633248  9.540000e+14 2024-12-03 14:39:00   \n",
      "4  52200758791     True  0.662274  7.470000e+14 2024-12-03 14:39:00   \n",
      "\n",
      "        date  sentiment_value sentiment_class  \n",
      "0 2024-12-03             76.0   Extreme Greed  \n",
      "1 2024-12-03             76.0   Extreme Greed  \n",
      "2 2024-12-03             76.0   Extreme Greed  \n",
      "3 2024-12-03             76.0   Extreme Greed  \n",
      "4 2024-12-03             76.0   Extreme Greed  \n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Load and Merge ---\n",
    "\n",
    "# Import the necessary functions (assuming 'load_and_merge_data' \n",
    "# is defined in this cell or in an imported file like 'Helper_Functions.py')\n",
    "# from Helper_Functions import load_and_merge_data \n",
    "\n",
    "print(\"--- Starting Step 1: Load and Merge Data ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define file paths for the datasets.\n",
    "# Using raw strings (r\"...\") is a best practice on Windows to prevent\n",
    "# backslashes from being interpreted as escape characters.\n",
    "SENTIMENT_FILE = r\"C:\\Users\\lenovo\\Downloads\\fear_greed_index.csv\"\n",
    "TRADER_FILE = r\"C:\\Users\\lenovo\\Downloads\\historical_data.csv\"\n",
    "# ---------------------\n",
    "\n",
    "# Execute the data loading and preprocessing pipeline\n",
    "# This function (defined previously) will load, clean, merge, \n",
    "# and filter the data.\n",
    "df_merged = load_and_merge_data(SENTIMENT_FILE, TRADER_FILE)\n",
    "\n",
    "# --- Post-Load Validation ---\n",
    "# It's crucial to check if the function successfully returned a DataFrame.\n",
    "# If `df_merged` is None, it means an error occurred during loading \n",
    "# (e.g., FileNotFoundError or no data left after filtering).\n",
    "if df_merged is not None:\n",
    "    # Confirmation message for a successful run\n",
    "    print(\"\\nSuccessfully loaded and merged data.\")\n",
    "    \n",
    "    # Display the first 5 rows of the final DataFrame\n",
    "    # This is a quick \"sanity check\" to ensure the merge and \n",
    "    # new columns (like 'sentiment_value') look correct.\n",
    "    print(df_merged.head())\n",
    "else:\n",
    "    # Error message if the previous step failed\n",
    "    print(\"\\nData loading and merging failed. Please check file paths and function logs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163848da",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dba5a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Step 2: Feature Engineering ---\n",
      "Engineering features...\n",
      "Feature engineering complete.\n",
      "\n",
      "Successfully engineered features.\n",
      "       is_profitable  sentiment_value  Size USD  trader_win_rate_past_5  \\\n",
      "21020              0             72.0    641.22                1.000000   \n",
      "21021              0             72.0  24884.62                0.500000   \n",
      "21024              1             72.0  10219.21                0.333333   \n",
      "21025              1             72.0   6752.75                0.500000   \n",
      "21023              1             72.0   8986.03                0.600000   \n",
      "\n",
      "       trader_avg_pnl_past_5  sentiment_class_Extreme Fear  \\\n",
      "21020             245.428491                         False   \n",
      "21021             116.099246                         False   \n",
      "21024             -93.934880                         False   \n",
      "21025             -62.794722                         False   \n",
      "21023             -46.472958                         False   \n",
      "\n",
      "       sentiment_class_Extreme Greed  sentiment_class_Fear  \\\n",
      "21020                          False                 False   \n",
      "21021                          False                 False   \n",
      "21024                          False                 False   \n",
      "21025                          False                 False   \n",
      "21023                          False                 False   \n",
      "\n",
      "       sentiment_class_Greed  sentiment_class_Neutral  ...  Coin_ZEREBRO  \\\n",
      "21020                   True                    False  ...         False   \n",
      "21021                   True                    False  ...         False   \n",
      "21024                   True                    False  ...         False   \n",
      "21025                   True                    False  ...         False   \n",
      "21023                   True                    False  ...         False   \n",
      "\n",
      "       Coin_ZETA  Coin_ZK  Coin_ZORA  Coin_ZRO  Coin_kBONK  Coin_kFLOKI  \\\n",
      "21020      False    False      False     False       False        False   \n",
      "21021      False    False      False     False       False        False   \n",
      "21024      False    False      False     False       False        False   \n",
      "21025      False    False      False     False       False        False   \n",
      "21023      False    False      False     False       False        False   \n",
      "\n",
      "       Coin_kNEIRO  Coin_kPEPE  Coin_kSHIB  \n",
      "21020        False       False       False  \n",
      "21021        False       False       False  \n",
      "21024        False       False       False  \n",
      "21025        False       False       False  \n",
      "21023        False       False       False  \n",
      "\n",
      "[5 rows x 232 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Engineer Features ---\n",
    "\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> Optional[Tuple[pd.DataFrame, List[str], str]]:\n",
    "    \"\"\"\n",
    "    Engineers predictive features and creates the target variable for modeling.\n",
    "\n",
    "    This function takes the merged trade and sentiment DataFrame and performs\n",
    "    several critical feature engineering steps:\n",
    "    1.  Sorts data by time to ensure chronological order for historical features.\n",
    "    2.  Creates the binary target variable 'is_profitable' (1 if PnL > 0, else 0).\n",
    "    3.  Calculates historical \"lagged\" features for each trader, such as\n",
    "        their win rate and average PnL over their last 5 trades. This is\n",
    "        done using a grouped shift-rolling operation to prevent data leakage.\n",
    "    4.  Performs one-hot encoding on categorical features ('sentiment_class', \n",
    "        'Side', 'Coin') to convert them into a machine-readable format.\n",
    "    5.  Defines the final list of features (X) and the target (y) and cleans\n",
    "        up any rows with NaNs resulting from the rolling operations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): \n",
    "            The merged DataFrame from `load_and_merge_data`. \n",
    "            Must contain 'Timestamp', 'Closed PnL', 'Account', \n",
    "            'sentiment_class', 'Side', and 'Coin'.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Tuple[pd.DataFrame, List[str], str]]: \n",
    "            A tuple containing:\n",
    "            1.  `df_final` (pd.DataFrame): The DataFrame with all features and target.\n",
    "            2.  `features` (List[str]): The list of all feature column names.\n",
    "            3.  `target` (str): The name of the target column ('is_profitable').\n",
    "            Returns (None, None, None) if the DataFrame becomes empty\n",
    "            after engineering.\n",
    "    \"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    \n",
    "    # Sort by 'Timestamp' to ensure chronological order.\n",
    "    # This is essential for calculating rolling features correctly.\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "    \n",
    "    # --- 1. Create Target Variable (y) ---\n",
    "    \n",
    "    # Create the binary target variable: 1 for a profitable trade, 0 otherwise.\n",
    "    df['is_profitable'] = (df['Closed PnL'] > 0).astype(int)\n",
    "    \n",
    "    # --- 2. Create Historical Trader Features (Lagged Features) ---\n",
    "    \n",
    "    # Group by 'Account' to ensure a trader's history is only calculated from\n",
    "    # their *own* past trades, preventing data leakage between traders.\n",
    "    \n",
    "    # --- Trader Win Rate (Past 5 Trades) ---\n",
    "    # .shift(1): Looks at the *previous* trade's profitability (prevents using\n",
    "    #            the current trade's outcome to predict itself).\n",
    "    # .rolling(window=5, min_periods=1): Creates a 5-trade rolling window.\n",
    "    #                                     min_periods=1 allows calculation\n",
    "    #                                     for a trader's first few trades.\n",
    "    # .mean(): Calculates the average (win rate) within that window.\n",
    "    df['trader_win_rate_past_5'] = df.groupby('Account')['is_profitable'] \\\n",
    "                                     .shift(1) \\\n",
    "                                     .rolling(window=5, min_periods=1) \\\n",
    "                                     .mean()\n",
    "                                     \n",
    "    # --- Trader Average PnL (Past 5 Trades) ---\n",
    "    # Same logic, but calculates the average 'Closed PnL' instead of win rate.\n",
    "    # This captures the *magnitude* of their recent wins/losses.\n",
    "    df['trader_avg_pnl_past_5'] = df.groupby('Account')['Closed PnL'] \\\n",
    "                                    .shift(1) \\\n",
    "                                    .rolling(window=5, min_periods=1) \\\n",
    "                                    .mean()\n",
    "\n",
    "    # --- 3. Handle Categorical Features (One-Hot Encoding) ---\n",
    "    \n",
    "    # Convert categorical text data into numeric format.\n",
    "    # 'dummy_na=False' ensures we don't create a separate column for NaN values.\n",
    "    df = pd.get_dummies(df, columns=['sentiment_class', 'Side', 'Coin'], \n",
    "                        dummy_na=False)\n",
    "\n",
    "    # --- 4. Finalize Feature List ---\n",
    "    \n",
    "    # Define the core numeric features\n",
    "    features = [\n",
    "        'sentiment_value',      # The raw Fear & Greed score\n",
    "        'Size USD',             # The size of the trade\n",
    "        'trader_win_rate_past_5', # Our engineered historical feature\n",
    "        'trader_avg_pnl_past_5'   # Our other engineered historical feature\n",
    "    ]\n",
    "    \n",
    "    # Automatically find and add all the new one-hot encoded (OHE) columns\n",
    "    ohe_cols = [col for col in df.columns if 'sentiment_class_' in col or \n",
    "                'Side_' in col or 'Coin_' in col]\n",
    "    features.extend(ohe_cols)\n",
    "    \n",
    "    # --- 5. Clean up ---\n",
    "    \n",
    "    # The rolling operations created NaNs for the first few trades\n",
    "    # (where no history was available). We must drop these rows.\n",
    "    df_final = df.dropna(subset=features)\n",
    "    \n",
    "    if df_final.empty:\n",
    "        print(\"Error: No data remaining after feature engineering (dropna).\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Define the target column name\n",
    "    target = 'is_profitable'\n",
    "    \n",
    "    print(\"Feature engineering complete.\")\n",
    "    return df_final, features, target\n",
    "\n",
    "# --- Call the function ---\n",
    "\n",
    "print(\"\\n--- Starting Step 2: Feature Engineering ---\")\n",
    "\n",
    "# Check if the 'df_merged' DataFrame from Cell 1 exists and is not None\n",
    "if 'df_merged' in locals() and df_merged is not None:\n",
    "    \n",
    "    # Run the feature engineering pipeline\n",
    "    df_final, features, target = engineer_features(df_merged)\n",
    "    \n",
    "    # Check if the engineering was successful\n",
    "    if df_final is not None:\n",
    "        print(f\"\\nSuccessfully engineered features.\")\n",
    "        # Optional: Print the feature list for review\n",
    "        # print(f\"Identified features: {features}\")\n",
    "        \n",
    "        # Display the first 5 rows of the final dataset (target + features)\n",
    "        # This is a sanity check to see the final data going into the model.\n",
    "        print(df_final[[target] + features].head())\n",
    "else:\n",
    "    # Error if the required input DataFrame is missing\n",
    "    print(\"Error: 'df_merged' not found. Please run Cell 1 successfully first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd6614",
   "metadata": {},
   "source": [
    "Split & Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4eab08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Step 3: Split and Scale Data ---\n",
      "Splitting and scaling data...\n",
      "Training set size: 83525 trades\n",
      "Test set size: 20882 trades\n",
      "Data successfully split and scaled.\n",
      "\n",
      "'data_packages' dictionary is ready.\n",
      "It contains 'unscaled', 'scaled', 'scaler', and 'feature_list'.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Split and Scale Data ---\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "def split_and_scale_data(df: pd.DataFrame, features: List[str], target: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Splits time-series data, aligns features, and scales for modeling.\n",
    "\n",
    "    This function performs the crucial data preparation step before modeling.\n",
    "    It takes the final engineered DataFrame and:\n",
    "    1.  Splits it into training (80%) and testing (20%) sets based on time.\n",
    "        **Crucially, it does not shuffle the data**, preserving the\n",
    "        chronological order, which is essential for time-series validation.\n",
    "    2.  Aligns the feature columns between train and test sets. This handles\n",
    "        any categorical features (from one-hot encoding) that might be\n",
    "        present in one set but not the other, preventing errors during\n",
    "        prediction.\n",
    "    3.  Applies `StandardScaler` to the features. It fits the scaler *only*\n",
    "        on the training data and then uses it to transform both the\n",
    "        training and test sets. This prevents data leakage.\n",
    "    4.  Packages all resulting data (scaled, unscaled, scaler object,\n",
    "        and feature list) into a dictionary for easy access in later steps.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): \n",
    "            The final, cleaned DataFrame from `engineer_features`.\n",
    "        features (List[str]): \n",
    "            The list of all feature column names to be used as model inputs (X).\n",
    "        target (str): \n",
    "            The name of the target variable column (y).\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: \n",
    "            A dictionary (`data_packages`) containing all data needed for modeling:\n",
    "            - \"unscaled\": (X_train, y_train, X_test, y_test)\n",
    "            - \"scaled\": (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "            - \"scaler\": The fitted StandardScaler object.\n",
    "            - \"feature_list\": The ordered list of feature names.\n",
    "            Returns `None` if the train or test split results in an empty DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Splitting and scaling data...\")\n",
    "    \n",
    "    # --- 1. Time-Series Split (80% train, 20% test) ---\n",
    "    \n",
    "    # Calculate the index for an 80/20 split\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    \n",
    "    # Split the data chronologically.\n",
    "    # .iloc[:train_size] takes the first 80% (past data) for training.\n",
    "    train_df = df.iloc[:train_size]\n",
    "    # .iloc[train_size:] takes the last 20% (future data) for testing.\n",
    "    test_df = df.iloc[train_size:]\n",
    "\n",
    "    # Validation check to ensure both splits have data\n",
    "    if train_df.empty or test_df.empty:\n",
    "        print(\"Error: Train or test split resulted in an empty dataframe.\")\n",
    "        return None\n",
    "\n",
    "    # Separate features (X) and target (y) for both sets\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[target]\n",
    "    X_test = test_df[features]\n",
    "    y_test = test_df[target]\n",
    "\n",
    "    print(f\"Training set size: {len(X_train)} trades\")\n",
    "    print(f\"Test set size: {len(X_test)} trades\")\n",
    "\n",
    "    # --- 2. Align Columns ---\n",
    "    \n",
    "    # This is critical if a rare category (e.g., a new 'Coin')\n",
    "    # appears in the test set but was *not* in the training set.\n",
    "    train_cols = X_train.columns\n",
    "    test_cols = X_test.columns\n",
    "    \n",
    "    # Find columns in training set but not in test set\n",
    "    missing_in_test = set(train_cols) - set(test_cols)\n",
    "    for c in missing_in_test:\n",
    "        X_test[c] = 0 # Add missing col to test set, fill with 0\n",
    "        \n",
    "    # Find columns in test set but not in training set\n",
    "    missing_in_train = set(test_cols) - set(train_cols)\n",
    "    for c in missing_in_train:\n",
    "        X_train[c] = 0 # Add missing col to train set, fill with 0\n",
    "        \n",
    "    # Ensure the final column order is identical for both sets\n",
    "    X_test = X_test[train_cols]\n",
    "\n",
    "    # --- 3. Scaling ---\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler *ONLY* on the training data.\n",
    "    # This calculates the mean and std dev of the training data.\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Transform the test data using the *same mean and std dev*\n",
    "    # from the training data. This prevents \"leaking\" test set info.\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Data successfully split and scaled.\")\n",
    "    \n",
    "    # --- 4. Package for easy use ---\n",
    "    \n",
    "    # Store all data versions in a dictionary for convenient passing\n",
    "    # to the modeling functions.\n",
    "    data_packages = {\n",
    "        # Unscaled data (for tree models like Random Forest, XGBoost)\n",
    "        \"unscaled\": (X_train, y_train, X_test, y_test),\n",
    "        # Scaled data (for models like LogReg, SVM, Deep Learning)\n",
    "        \"scaled\": (X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "        # The fitted scaler (to transform new data in the future)\n",
    "        \"scaler\": scaler,\n",
    "        # The master list of features (for feature importance)\n",
    "        \"feature_list\": train_cols \n",
    "    }\n",
    "    \n",
    "    return data_packages\n",
    "\n",
    "# --- Call the function ---\n",
    "\n",
    "print(\"\\n--- Starting Step 3: Split and Scale Data ---\")\n",
    "\n",
    "# Check if the 'df_final' DataFrame from Cell 2 exists\n",
    "if 'df_final' in locals() and df_final is not None:\n",
    "    \n",
    "    # Execute the splitting and scaling function\n",
    "    data_packages = split_and_scale_data(df_final, features, target)\n",
    "    \n",
    "    # Check for successful execution\n",
    "    if data_packages is not None:\n",
    "        print(\"\\n'data_packages' dictionary is ready.\")\n",
    "        print(\"It contains 'unscaled', 'scaled', 'scaler', and 'feature_list'.\")\n",
    "else:\n",
    "    # Error if the required input DataFrame is missing\n",
    "    print(\"Error: 'df_final' not found. Please run Cell 1 and Cell 2 successfully first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f35d5",
   "metadata": {},
   "source": [
    "Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b24be18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Step 4: Model Training & Evaluation ---\n",
      "Training and evaluating all models...\n",
      "--- Training Logistic Regression ---\n",
      "--- Logistic Regression Complete ---\n",
      "--- Training Random Forest ---\n",
      "--- Random Forest Complete ---\n",
      "--- Training XGBoost ---\n",
      "--- XGBoost Complete ---\n",
      "--- Training SVM (SVC) ---\n",
      "--- SVM (SVC) Complete ---\n",
      "--- Training Deep Learning (MLP) ---\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--- Deep Learning (MLP) Complete ---\n",
      "\n",
      "--- Model Comparison Results ---\n",
      "                     Accuracy  Precision    Recall  F1-Score   ROC-AUC\n",
      "Model                                                                 \n",
      "XGBoost              0.952639   0.954180  0.983162  0.968454  0.983070\n",
      "Logistic Regression  0.956087   0.961548  0.979794  0.970585  0.980927\n",
      "SVM (SVC)            0.929269   0.927976  0.980442  0.953488  0.976186\n",
      "Random Forest        0.934202   0.932166  0.982514  0.956678  0.975726\n",
      "Deep Learning (MLP)  0.936261   0.954633  0.959394  0.957008  0.958232\n",
      "\n",
      "Storing results for Step 5...\n",
      "Stored 'results_df' (DataFrame)\n",
      "Stored 'trained_models' (dict)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Train and Evaluate Top 5 Models ---\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.metrics import (accuracy_score, precision_score, \n",
    "                             recall_score, f1_score, roc_auc_score)\n",
    "\n",
    "# Import all the model types\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# Import Deep Learning (TensorFlow/Keras) libraries\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_deep_learning_model(input_shape: int) -> Model:\n",
    "    \"\"\"\n",
    "    Creates a simple Keras Sequential (MLP) model for binary classification.\n",
    "\n",
    "    This defines a basic feedforward neural network architecture:\n",
    "    Input -> Dense(64, relu) -> Dropout(0.3) -> Dense(32, relu) -> \n",
    "    Dropout(0.3) -> Output(1, sigmoid)\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): \n",
    "            The number of input features (e.g., X_train.shape[1]).\n",
    "\n",
    "    Returns:\n",
    "        Model: \n",
    "            A compiled, untrained Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer + first hidden layer. 'input_shape' is only needed for the first layer.\n",
    "    model.add(Dense(64, activation='relu', input_shape=(input_shape,)))\n",
    "    \n",
    "    # Dropout layer: Randomly sets 30% of input units to 0 during training\n",
    "    # to help prevent overfitting.\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Second hidden layer\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Output layer: 1 neuron with a 'sigmoid' activation function\n",
    "    # which squashes the output to a probability (0 to 1).\n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  # 'binary_crossentropy' is the standard loss function for\n",
    "                  # two-class (binary) classification problems.\n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_models(data_packages: Dict[str, Any]) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Trains, evaluates, and compares 5 different ML/DL models.\n",
    "\n",
    "    This function performs a \"bake-off\" to see which model architecture\n",
    "    performs best on the data \"out of the box.\"\n",
    "    \n",
    "    It trains:\n",
    "    1. Logistic Regression\n",
    "    2. Random Forest Classifier\n",
    "    3. XGBoost Classifier\n",
    "    4. Support Vector Machine (SVC)\n",
    "    5. A simple Deep Learning (MLP) model\n",
    "\n",
    "    It correctly uses scaled data for models that require it (LogReg, SVM, DL)\n",
    "    and unscaled data for tree-based models (RF, XGB).\n",
    "    \n",
    "    Args:\n",
    "        data_packages (Dict[str, Any]): \n",
    "            The dictionary created by `split_and_scale_data`. Must contain\n",
    "            keys \"unscaled\" and \"scaled\", each holding a tuple of\n",
    "            (X_train, y_train, X_test, y_test).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "            1. `results_df` (pd.DataFrame): A DataFrame with models as the\n",
    "               index and their performance metrics (Accuracy, Precision,\n",
    "               Recall, F1, ROC-AUC) as columns.\n",
    "            2. `trained_models` (Dict[str, Any]): A dictionary where keys\n",
    "               are model names and values are the fitted model objects.\n",
    "    \"\"\"\n",
    "    print(\"Training and evaluating all models...\")\n",
    "    \n",
    "    # --- 1. Unpack all our data ---\n",
    "    \n",
    "    # Unscaled data (for tree-based models: Random Forest, XGBoost)\n",
    "    X_train, y_train, X_test, y_test = data_packages['unscaled']\n",
    "    \n",
    "    # Scaled data (for LogReg, SVM, and Deep Learning)\n",
    "    X_train_sc, _, X_test_sc, _ = data_packages['scaled']\n",
    "    \n",
    "    # --- 2. Define the models ---\n",
    "    models = {\n",
    "        \"Logistic Regression\": {\n",
    "            \"model\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "            \"use_scaled\": True # Needs scaled data for convergence\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            # n_jobs=-1 uses all available CPU cores for training\n",
    "            \"model\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "            \"use_scaled\": False # Tree models are not sensitive to feature scale\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            # eval_metric='logloss' is standard for binary classification\n",
    "            \"model\": xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False, \n",
    "                                     random_state=42),\n",
    "            \"use_scaled\": False # Tree models are not sensitive to feature scale\n",
    "        },\n",
    "        \"SVM (SVC)\": {\n",
    "            # probability=True is required to get predict_proba()\n",
    "            # which is needed for ROC-AUC score. Can be slow.\n",
    "            \"model\": SVC(probability=True, random_state=42), \n",
    "            \"use_scaled\": True # Needs scaled data\n",
    "        },\n",
    "        \"Deep Learning (MLP)\": {\n",
    "            # Build the model, passing in the number of features\n",
    "            \"model\": build_deep_learning_model(X_train_sc.shape[1]),\n",
    "            \"use_scaled\": True # Needs scaled data\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = [] # A list to store metric dicts\n",
    "    trained_models = {} # A dict to store the fitted models\n",
    "    \n",
    "    # --- 3. Loop, Train, and Evaluate ---\n",
    "    for name, m in models.items():\n",
    "        print(f\"--- Training {name} ---\")\n",
    "        \n",
    "        # Select the correct dataset (scaled or unscaled)\n",
    "        if m['use_scaled']:\n",
    "            X_t, X_te = X_train_sc, X_test_sc\n",
    "        else:\n",
    "            X_t, X_te = X_train, X_test\n",
    "            \n",
    "        # --- Train ---\n",
    "        if name == \"Deep Learning (MLP)\":\n",
    "            # Keras/DL models have a different training/prediction API\n",
    "            \n",
    "            # EarlyStopping stops training when validation loss stops\n",
    "            # improving, preventing overfitting and saving time.\n",
    "            es = EarlyStopping(monitor='val_loss', patience=10, \n",
    "                               restore_best_weights=True)\n",
    "            \n",
    "            m['model'].fit(X_t, y_train,\n",
    "                           epochs=100,       # Max number of passes over the data\n",
    "                           batch_size=64,\n",
    "                           # Use 10% of training data as a validation set\n",
    "                           # for EarlyStopping.\n",
    "                           validation_split=0.1, \n",
    "                           callbacks=[es],\n",
    "                           verbose=0) # verbose=0 suppresses training logs\n",
    "            \n",
    "            # Predict probabilities\n",
    "            y_pred_proba = m['model'].predict(X_te).ravel() # .ravel() flattens (n, 1) to (n,)\n",
    "            # Convert probabilities to binary classes (0 or 1)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        else:\n",
    "            # Standard sklearn model API\n",
    "            m['model'].fit(X_t, y_train)\n",
    "            \n",
    "            # Predict binary classes\n",
    "            y_pred = m['model'].predict(X_te)\n",
    "            # Predict probabilities. [:, 1] selects the probability \n",
    "            # of the *positive class* (class 1).\n",
    "            y_pred_proba = m['model'].predict_proba(X_te)[:, 1] \n",
    "            \n",
    "        # --- Evaluate ---\n",
    "        # Calculate all key classification metrics\n",
    "        # zero_division=0 prevents warnings if a class is never predicted\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1,\n",
    "            \"ROC-AUC\": roc_auc\n",
    "        })\n",
    "        \n",
    "        # Store the fitted model object for later use\n",
    "        trained_models[name] = m['model'] \n",
    "        print(f\"--- {name} Complete ---\")\n",
    "\n",
    "    # Convert the list of results into a clean DataFrame\n",
    "    results_df = pd.DataFrame(results).set_index(\"Model\")\n",
    "    return results_df, trained_models\n",
    "\n",
    "# --- Call the function ---\n",
    "\n",
    "print(\"\\n--- Starting Step 4: Model Training & Evaluation ---\")\n",
    "\n",
    "# Check if the 'data_packages' dictionary from Cell 3 exists\n",
    "if 'data_packages' in locals():\n",
    "    \n",
    "    # This line runs all the training and evaluation\n",
    "    results_df, trained_models = train_and_evaluate_models(data_packages)\n",
    "    \n",
    "    print(\"\\n--- Model Comparison Results ---\")\n",
    "    \n",
    "    # Sort by ROC-AUC (Area Under the Curve), as it's a robust\n",
    "    # metric for classification performance, especially with\n",
    "    # potentially imbalanced classes.\n",
    "    print(results_df.sort_values(by='ROC-AUC', ascending=False))\n",
    "    \n",
    "    # --- Store results for the next notebook cell ---\n",
    "    # '%store' is a Jupyter \"magic command\" that saves variables\n",
    "    # to be loaded in other sessions or notebooks.\n",
    "    print(\"\\nStoring results for Step 5...\")\n",
    "    %store results_df\n",
    "    %store trained_models\n",
    "    \n",
    "else:\n",
    "    # Error if the required input dictionary is missing\n",
    "    print(\"Error: 'data_packages' not found. Please run all previous cells successfully first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da2c26",
   "metadata": {},
   "source": [
    "Hyperparameter tuning for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b9d269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Step 5: Hyperparameter Tuning & Final Analysis ---\n",
      "\n",
      "--- Hyperparameter Tuning Best Model (XGBoost) ---\n",
      "Running RandomizedSearch... This may take a few minutes.\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "\n",
      "Tuning complete.\n",
      "Best parameters found: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0.5, 'colsample_bytree': 0.6}\n",
      "Best ROC-AUC score during tuning: 0.9679\n",
      "\n",
      "--- Tuned Model Performance on Test Set ---\n",
      "Tuned Accuracy: 0.9528\n",
      "Tuned Precision: 0.9541\n",
      "Tuned Recall: 0.9835\n",
      "Tuned F1-Score: 0.9686\n",
      "Tuned ROC-AUC: 0.9841\n",
      "\n",
      "--- Top 10 Most Important Features ---\n",
      "trader_win_rate_past_5    0.600210\n",
      "trader_avg_pnl_past_5     0.061947\n",
      "Coin_@4                   0.027504\n",
      "Coin_NIL                  0.015013\n",
      "Coin_ETH                  0.013166\n",
      "Coin_PENGU                0.012340\n",
      "Coin_HYPE                 0.011256\n",
      "Coin_ZRO                  0.008396\n",
      "Coin_@107                 0.008261\n",
      "Coin_SOL                  0.007726\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: Hyperparameter Tuning & Final Analysis ---\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from typing import Dict, Any\n",
    "from sklearn.metrics import (accuracy_score, precision_score, \n",
    "                             recall_score, f1_score, roc_auc_score)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def tune_best_model(data_packages: Dict[str, Any]) -> xgb.XGBClassifier:\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning on the XGBoost model.\n",
    "\n",
    "    This function uses `RandomizedSearchCV` to efficiently search a \n",
    "    predefined parameter grid. Crucially, it uses `TimeSeriesSplit` \n",
    "    for cross-validation to respect the chronological order of the data, \n",
    "    preventing data leakage and providing a realistic performance estimate.\n",
    "\n",
    "    It optimizes for 'roc_auc', our primary metric for classification\n",
    "    performance.\n",
    "\n",
    "    Args:\n",
    "        data_packages (Dict[str, Any]): \n",
    "            The dictionary from `split_and_scale_data`. Must contain\n",
    "            the \"unscaled\" key with (X_train, y_train, X_test, y_test).\n",
    "\n",
    "    Returns:\n",
    "        xgb.XGBClassifier: \n",
    "            The best-performing, fitted XGBoost model object found \n",
    "            during the randomized search.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Hyperparameter Tuning Best Model (XGBoost) ---\")\n",
    "    \n",
    "    # --- 1. Unpack Data ---\n",
    "    # XGBoost (a tree-based model) does not require scaled data.\n",
    "    X_train, y_train, X_test, y_test = data_packages['unscaled']\n",
    "    \n",
    "    # --- 2. Define the Model and Parameter Grid ---\n",
    "    \n",
    "    # Initialize the base XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(eval_metric='logloss', \n",
    "                                use_label_encoder=False, \n",
    "                                random_state=42)\n",
    "    \n",
    "    # Define the \"search space\" for the tuner.\n",
    "    # These are the hyperparameters we want to test.\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 500],      # Number of boosting rounds (trees)\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],   # Step size shrinkage to prevent overfitting\n",
    "        'max_depth': [3, 5, 7, 10],                # Maximum depth of an individual tree\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],       # Fraction of features used for each tree\n",
    "        'subsample': [0.6, 0.8, 1.0],              # Fraction of data samples used for each tree\n",
    "        'gamma': [0, 0.1, 0.5, 1]                  # Minimum loss reduction to make a split (regularization)\n",
    "    }\n",
    "    \n",
    "    # --- 3. Set up Time-Series Cross-Validation ---\n",
    "    \n",
    "    # This is *critical* for time-series data.\n",
    "    # It creates folds like:\n",
    "    # Fold 1: train=[0], test=[1]\n",
    "    # Fold 2: train=[0, 1], test=[2]\n",
    "    # This ensures we always train on the past and validate on the future.\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # --- 4. Set up Randomized Search ---\n",
    "    \n",
    "    # RandomizedSearchCV is faster than GridSearchCV as it samples \n",
    "    # a fixed number of parameter combinations (n_iter).\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,                # The model to tune\n",
    "        param_distributions=param_grid,     # The hyperparameter grid to search\n",
    "        n_iter=25,                          # Number of random combinations to try\n",
    "        scoring='roc_auc',                  # The metric to optimize\n",
    "        cv=tscv,                            # Use our time-series cross-validator\n",
    "        n_jobs=-1,                          # Use all available CPU cores\n",
    "        verbose=1,                          # Print progress updates\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # --- 5. Run the Search ---\n",
    "    print(\"Running RandomizedSearch... This may take a few minutes.\")\n",
    "    # This fits the 'random_search' object, which in turn\n",
    "    # fits 'n_iter' * 'n_splits' models.\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nTuning complete.\")\n",
    "    # Display the best hyperparameter combination found\n",
    "    print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "    # Display the mean cross-validated score of the best estimator\n",
    "    print(f\"Best ROC-AUC score during tuning: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # --- 6. Evaluate the Final Tuned Model ---\n",
    "    \n",
    "    # Get the best model identified by the search\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    # Evaluate this single best model on the *held-back test set*\n",
    "    # This is our final, unbiased measure of performance.\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\n--- Tuned Model Performance on Test Set ---\")\n",
    "    print(f\"Tuned Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Tuned Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    print(f\"Tuned Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    print(f\"Tuned F1-Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    print(f\"Tuned ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def show_feature_importance(model, data_packages: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Displays the top 10 most important features from the trained model.\n",
    "\n",
    "    This function is most effective for tree-based models (like \n",
    "    Random Forest or XGBoost) that have a `feature_importances_` attribute.\n",
    "\n",
    "    Args:\n",
    "        model: \n",
    "            A fitted model object (e.g., the returned `best_model` \n",
    "            from `tune_best_model`).\n",
    "        data_packages (Dict[str, Any]): \n",
    "            The dictionary from `split_and_scale_data`, used here to\n",
    "            retrieve the \"feature_list\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the ordered list of feature names\n",
    "        feature_list = data_packages['feature_list']\n",
    "        # Get the importance scores from the fitted model\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Create a pandas Series for easy sorting and display\n",
    "        feature_importance_df = pd.Series(\n",
    "            importances, \n",
    "            index=feature_list\n",
    "        ).sort_values(ascending=False) # Sort from most to least important\n",
    "\n",
    "        print(\"\\n--- Top 10 Most Important Features ---\")\n",
    "        # Display the top 10 features\n",
    "        print(feature_importance_df.head(10))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get feature importances: {e}\")\n",
    "        print(\"Note: This is easiest for tree-based models like XGBoost/Random Forest.\")\n",
    "\n",
    "# --- Call the functions ---\n",
    "\n",
    "print(\"\\n--- Starting Step 5: Hyperparameter Tuning & Final Analysis ---\")\n",
    "\n",
    "# Check if the 'data_packages' dictionary from Cell 3 exists\n",
    "if 'data_packages' in locals():\n",
    "    \n",
    "    # 1. Run the tuning process\n",
    "    # This will find the best model and print its test set performance.\n",
    "    final_best_model = tune_best_model(data_packages)\n",
    "    \n",
    "    # 2. Show what the model learned\n",
    "    # This prints the feature importances, providing key insights.\n",
    "    show_feature_importance(final_best_model, data_packages)\n",
    "    \n",
    "else:\n",
    "    # Error if the required input dictionary is missing\n",
    "    print(\"Error: 'data_packages' not found. Please run all previous cells successfully first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
